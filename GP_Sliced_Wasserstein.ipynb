{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b0c59fc-a75e-4406-a0c1-1702e0060d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from fvgp import GP\n",
    "from fvgp.gp_kernels import exponential_kernel\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4dffb9e-572d-4399-9142-374b879ae529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess the Digits Dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalize the data to resemble probability distributions\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = (X_train[i] - np.min(X_train[i])) + 1e-8\n",
    "    X_train[i] = X_train[i] / np.sum(X_train[i])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = (X_test[i] - np.min(X_test[i])) + 1e-8\n",
    "    X_test[i] = X_test[i] / np.sum(X_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304a7d78-7f4f-41d9-8331-58a1a56f3f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCA directions: 20\n",
      "Total number of directions (PCA + random): 40\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute PCA Directions Covering 90% of Variance and Random Directions\n",
    "pca = PCA(n_components=0.90)\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Get PCA directions\n",
    "pca_directions = pca.components_  # Shape: (n_pca_directions, n_features)\n",
    "num_pca_directions = pca_directions.shape[0]\n",
    "\n",
    "# Generate the same number of random directions\n",
    "np.random.seed(42)  # For reproducibility\n",
    "random_directions = np.random.randn(num_pca_directions, X_train.shape[1])\n",
    "random_directions /= np.linalg.norm(random_directions, axis=1, keepdims=True)\n",
    "\n",
    "# Combine PCA and random directions\n",
    "directions = np.vstack((pca_directions, random_directions))  # Shape: (2*num_pca_directions, n_features)\n",
    "num_directions = directions.shape[0]\n",
    "\n",
    "print(f\"Number of PCA directions: {num_pca_directions}\")\n",
    "print(f\"Total number of directions (PCA + random): {num_directions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f27100-0163-499e-be39-b0d5b40e1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Project the Data onto the Directions\n",
    "X_train_proj = X_train.dot(directions.T)  # Shape: (n_train_samples, num_directions)\n",
    "X_test_proj = X_test.dot(directions.T)    # Shape: (n_test_samples, num_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96824ad5-3aa1-40eb-b334-c0a9af11873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compute Pairwise Sliced Wasserstein Distances and Cache Them\n",
    "def compute_sliced_wasserstein_matrix(X_proj1, X_proj2):\n",
    "    n1 = X_proj1.shape[0]\n",
    "    n2 = X_proj2.shape[0]\n",
    "    distance_matrix = np.zeros((n1, n2))\n",
    "    \n",
    "    for d in range(num_directions):\n",
    "        print(f\"Processing direction {d+1}/{num_directions}\", end='\\r')\n",
    "        X1_proj_d = X_proj1[:, d]\n",
    "        X2_proj_d = X_proj2[:, d]\n",
    "        \n",
    "        # Compute pairwise Wasserstein distances for this direction\n",
    "        dist_matrix_d = np.array([\n",
    "            [wasserstein_distance([X1_proj_d[i]], [X2_proj_d[j]]) for j in range(n2)] \n",
    "            for i in range(n1)\n",
    "        ])\n",
    "        \n",
    "        distance_matrix += dist_matrix_d\n",
    "    \n",
    "    # Average over all directions\n",
    "    distance_matrix /= num_directions\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c29975d-33a9-42b6-b220-f8e7b476e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sliced Wasserstein distance matrices...\n",
      "Processing direction 40/40\n",
      "Sliced Wasserstein distance matrices computed.\n"
     ]
    }
   ],
   "source": [
    "# Compute and cache the distance matrices\n",
    "print(\"Computing sliced Wasserstein distance matrices...\")\n",
    "distance_matrix_train_train = compute_sliced_wasserstein_matrix(X_train_proj, X_train_proj)\n",
    "distance_matrix_train_test = compute_sliced_wasserstein_matrix(X_train_proj, X_test_proj)\n",
    "distance_matrix_test_test = compute_sliced_wasserstein_matrix(X_test_proj, X_test_proj)\n",
    "print(\"\\nSliced Wasserstein distance matrices computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e395c9-cd48-46e8-9a24-6fd8dac28bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define the GP Kernel Function Using Precomputed Distance Matrices\n",
    "def SW_kernel(X1, X2, hyperparameters):\n",
    "    length_scale = hyperparameters[0]\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    if len(X1) == n_train and len(X2) == n_train:\n",
    "        K = exponential_kernel(distance_matrix_train_train, length_scale)\n",
    "    elif len(X1) == n_test and len(X2) == n_test:\n",
    "        K = exponential_kernel(distance_matrix_test_test, length_scale)\n",
    "    elif len(X1) == n_train and len(X2) == n_test:\n",
    "        K = exponential_kernel(distance_matrix_train_test, length_scale)\n",
    "    elif len(X1) == n_test and len(X2) == n_train:\n",
    "        K = exponential_kernel(distance_matrix_train_test.T, length_scale)\n",
    "    else:\n",
    "        # For any other cases, compute the distances on-the-fly\n",
    "        X1_proj = X1.dot(directions.T)\n",
    "        X2_proj = X2.dot(directions.T)\n",
    "        distance_matrix = compute_sliced_wasserstein_matrix(X1_proj, X2_proj)\n",
    "        K = exponential_kernel(distance_matrix, length_scale)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780a4ab3-3b7c-4ebf-ac5d-41a7b1d8f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize Hyperparameters and Bounds\n",
    "initial_length_scale = 1.0  # Initial guess for length scale\n",
    "init_hyperparameters = np.array([initial_length_scale])\n",
    "\n",
    "# Define bounds for the length scale (e.g., between 0.1 and 10)\n",
    "length_scale_bounds = np.array([[0.1, 10.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d72ac1-fed6-4ab1-973e-cb54afe62203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP models...\n",
      "Training GP model for class 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fvgp/mcmc.py:116: RuntimeWarning: overflow encountered in exp\n",
      "  metr_ratio = np.exp(prior_star + likelihood_star - prior - likelihood)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1...\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2...\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3...\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4...\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5...\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6...\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7...\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8...\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9...\n",
      "GP model for class 9 trained.\n",
      "\n",
      "All GP models trained.\n"
     ]
    }
   ],
   "source": [
    "# 7. Train GP Models Using One-vs-Rest Strategy\n",
    "gp_models = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "print(\"Training GP models...\")\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}...\")\n",
    "    \n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train == class_label).astype(float)\n",
    "    \n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=SW_kernel,\n",
    "        noise_variances=np.zeros(len(y_train_binary)) + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=1000,\n",
    "        tolerance=1e-3,\n",
    "    )\n",
    "\n",
    "    gp_models.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")\n",
    "\n",
    "print(\"All GP models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a0cc3b-718e-4f4e-bd25-8c447c8dd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define the Probit Link Function (Prefer over Logit, Gaussian Assumptions)\n",
    "def probit(mu, sigma2):\n",
    "    # Applies the probit function with variance adjustment.\n",
    "    adjusted_mu = mu / np.sqrt(1 + sigma2)\n",
    "    return norm.cdf(adjusted_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a86001-5b2c-4afd-93d9-604e84228cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Predict Probabilities Using the Trained GP Models\n",
    "def predict_probs(X_test, gp_models):\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store means and variances\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "    variances = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior_mean = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior_mean[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "        \n",
    "        # Compute the posterior variance for the test data\n",
    "        posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "        variance = posterior_cov[\"v(x)\"]  # Extract variances\n",
    "        variances[:, class_label] = variance.flatten()\n",
    "    \n",
    "    # Apply probit with variance to convert means and variances to probabilities\n",
    "    probabilities = probit(means, variances)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b850cf-ff67-4ce3-90d9-63bd2b24ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 99%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.90      1.00      0.95        18\n",
      "           2       1.00      1.00      1.00        18\n",
      "           3       1.00      1.00      1.00        18\n",
      "           4       1.00      1.00      1.00        18\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      1.00      1.00        18\n",
      "           7       1.00      1.00      1.00        18\n",
      "           8       1.00      0.89      0.94        18\n",
      "           9       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       0.99      0.99      0.99       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Predict Class Labels and Evaluate the Classifier\n",
    "probabilities = predict_probs(X_test, gp_models)\n",
    "\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'\\nAccuracy: {accuracy:.0f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdb47ea1-47e3-4e54-9021-59d9964efbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved distance_matrices/distance_matrix_train_train.npy\n",
      "Saved distance_matrices/distance_matrix_train_test.npy\n",
      "Saved distance_matrices/distance_matrix_test_test.npy\n"
     ]
    }
   ],
   "source": [
    "def save_matrix(matrix, filename):\n",
    "    np.save(filename, matrix)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "output_dir = \"distance_matrices\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "save_matrix(distance_matrix_train_train, os.path.join(output_dir, \"distance_matrix_train_train.npy\"))\n",
    "save_matrix(distance_matrix_train_test, os.path.join(output_dir, \"distance_matrix_train_test.npy\"))\n",
    "save_matrix(distance_matrix_test_test, os.path.join(output_dir, \"distance_matrix_test_test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58acd2-d634-4b54-a88e-b58ed89ebcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
