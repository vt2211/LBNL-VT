{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8efab03-62d4-4e8f-b153-d5be591558be",
   "metadata": {},
   "source": [
    "# Handwritten Digit Classification\n",
    "### Gaussian Process Classification\n",
    "\n",
    "#### Dataset Description ([Link to Data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html))\n",
    "\n",
    "Each entry corresponds to one hand-written digit on 8x8 pixels. The dataset contains 1797 samples, with about 180 samples for each of the 10 classes (0-9). A Gaussian Process Classification using the Jensen-Shannon Metric with an exponential kernels, where 10 models are trained for a One-v-Rest approach. We use PCA for direction optimizationand use both the probit link function to convert the regression to classes. This example should replicated without too much difficulty or time. Instead of training on global optimization, use an MCMC with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8a2c6c-4f8a-4e69-b3b0-bf4c1904d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from fvgp import GP\n",
    "from fvgp.gp_kernels import exponential_kernel\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b511908-2e5c-436c-87f4-aa26597b635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess the Digits Dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalize the data to resemble probability distributions\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = (X_train[i] - np.min(X_train[i])) + 1e-8\n",
    "    X_train[i] = X_train[i] / np.sum(X_train[i])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = (X_test[i] - np.min(X_test[i])) + 1e-8\n",
    "    X_test[i] = X_test[i] / np.sum(X_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ad6fd8-4c5e-440e-8921-2295a5501707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define KL and JS Divergence Functions\n",
    "def KL(p, q):\n",
    "    return np.sum(p * np.log(p / q + 1e-10))  # Added epsilon to avoid log(0)\n",
    "\n",
    "def JS_divergence(p, q):\n",
    "    M = 0.5 * (p + q)\n",
    "    return 0.5 * (KL(p, M) + KL(q, M))\n",
    "\n",
    "# 3. Compute JS Divergence Matrices with Caching\n",
    "def compute_JS_matrix(X1, X2):\n",
    "    n1 = X1.shape[0]\n",
    "    n2 = X2.shape[0]\n",
    "    JS_matrix = np.zeros((n1, n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            JS_matrix[i, j] = JS_divergence(X1[i], X2[j])\n",
    "    return JS_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a08a86d-820b-4dd1-a67e-dce59a6bc94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing JS divergence matrices...\n",
      "JS divergence matrices computed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing JS divergence matrices...\")\n",
    "JS_X_train = compute_JS_matrix(X_train, X_train)       # Training vs. Training\n",
    "JS_X_test = compute_JS_matrix(X_test, X_test)          # Testing vs. Testing\n",
    "JS_X_train_test = compute_JS_matrix(X_train, X_test)   # Training vs. Testing\n",
    "print(\"JS divergence matrices computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ef87bb-02ba-466c-ae50-a7cc57e0b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the GP Kernel Function Using Precomputed JS Matrices\n",
    "def JS_kernel(X1, X2, hyperparameters):\n",
    "    length_scale = hyperparameters[0]\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    if len(X1) == n_train and len(X2) == n_train:\n",
    "        K = exponential_kernel(JS_X_train, length_scale)\n",
    "    elif len(X1) == n_test and len(X2) == n_test:\n",
    "        K = exponential_kernel(JS_X_test, length_scale)\n",
    "    elif len(X1) == n_train and len(X2) == n_test:\n",
    "        K = exponential_kernel(JS_X_train_test, length_scale)\n",
    "    elif len(X1) == n_test and len(X2) == n_train:\n",
    "        K = exponential_kernel(JS_X_train_test.T, length_scale)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input sizes for X1 and X2.\")\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0abcb95-b7c0-4a64-ba69-5f0979d98996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Hyperparameters and Bounds\n",
    "initial_length_scale = 1.0\n",
    "init_hyperparameters = np.array([initial_length_scale])\n",
    "\n",
    "# Define bounds for the length scale\n",
    "length_scale_bounds = np.array([[0.1, 10.0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9e5637-c429-4f25-bc31-905c2c933ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP models...\n",
      "Training GP model for class 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fvgp/mcmc.py:116: RuntimeWarning: overflow encountered in exp\n",
      "  metr_ratio = np.exp(prior_star + likelihood_star - prior - likelihood)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP model for class 0 trained.\n",
      "Training GP model for class 1...\n",
      "GP model for class 1 trained.\n",
      "Training GP model for class 2...\n",
      "GP model for class 2 trained.\n",
      "Training GP model for class 3...\n",
      "GP model for class 3 trained.\n",
      "Training GP model for class 4...\n",
      "GP model for class 4 trained.\n",
      "Training GP model for class 5...\n",
      "GP model for class 5 trained.\n",
      "Training GP model for class 6...\n",
      "GP model for class 6 trained.\n",
      "Training GP model for class 7...\n",
      "GP model for class 7 trained.\n",
      "Training GP model for class 8...\n",
      "GP model for class 8 trained.\n",
      "Training GP model for class 9...\n",
      "GP model for class 9 trained.\n",
      "All GP models trained.\n"
     ]
    }
   ],
   "source": [
    "# 6. Train GP Models Using One-vs-Rest Strategy\n",
    "gp_models = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "print(\"Training GP models...\")\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}...\")\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train == class_label).astype(float)\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=JS_kernel,\n",
    "        noise_variances=np.zeros(len(y_train_binary)) + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model (optimize hyperparameters)\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=1000,\n",
    "        tolerance=1e-3,\n",
    "    )\n",
    "\n",
    "    gp_models.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\")\n",
    "\n",
    "print(\"All GP models trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "126c3df7-221d-4377-98e6-d0a951709208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define the Probit Link Function (Prefer over Logit, Gaussian Assumptions)\n",
    "def probit(mu, sigma2):\n",
    "    # Applies the probit function with variance adjustment.\n",
    "    adjusted_mu = mu / np.sqrt(1 + sigma2)\n",
    "    return norm.cdf(adjusted_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4017a3f8-efdd-4f1e-b429-662241d64c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Predict Probabilities Using the Trained GP Models\n",
    "def predict_probs(X_test, gp_models):\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store means and variances\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "    variances = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior_mean = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior_mean[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "        \n",
    "        # Compute the posterior variance for the test data\n",
    "        posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "        variance = posterior_cov[\"v(x)\"]  # Extract variances\n",
    "        variances[:, class_label] = variance.flatten()\n",
    "    \n",
    "    # Apply probit with variance to convert means and variances to probabilities\n",
    "    probabilities = probit_with_variance(means, variances)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73509166-76ca-4852-b394-006df8246e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 99%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       0.90      1.00      0.95        18\n",
      "           2       1.00      1.00      1.00        18\n",
      "           3       1.00      1.00      1.00        18\n",
      "           4       1.00      1.00      1.00        18\n",
      "           5       1.00      1.00      1.00        18\n",
      "           6       1.00      0.94      0.97        18\n",
      "           7       1.00      1.00      1.00        18\n",
      "           8       1.00      0.94      0.97        18\n",
      "           9       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           0.99       180\n",
      "   macro avg       0.99      0.99      0.99       180\n",
      "weighted avg       0.99      0.99      0.99       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Predict Class Labels and Evaluate the Classifier\n",
    "probabilities = predict_probs(X_test, gp_models)\n",
    "\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'\\nAccuracy: {accuracy:.0f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
