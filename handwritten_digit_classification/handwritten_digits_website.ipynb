{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce29299-b64f-494a-8b5a-4b8bb6e8f915",
   "metadata": {},
   "source": [
    "# Handwritten Digit Classification\n",
    "### Gaussian Process Classification\n",
    "\n",
    "#### Dataset Description ([Link to Data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html))\n",
    "\n",
    "Each entry corresponds to one hand-written digit on 8x8 pixels. The dataset contains 1797 samples, with about 180 samples for each of the 10 classes (0-9). A Gaussian Process Classification using the Sliced Wasserstein Distance with an exponential kernels, where 10 models are trained for a One-v-Rest approach. We use PCA for direction optimizationand use both the probit and logit link function to convert the regression to classes. This example should replicated withou too much difficulty or time. Instead of training on global optimization, we sparsify the data and use an MCMC with 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146ed353-774b-4b3b-9baf-aef2d5b31e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gpcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1085846e-2b28-4e17-aae6-d1ede099230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from fvgp import GP\n",
    "from fvgp.gp_kernels import exponential_kernel\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6017db6-0786-4375-a6db-7176a55839a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess the Digits Dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3863ac-d3e7-44ad-8975-b5ae8c474737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Sparse Datasets by Selecting Every 4th Sample\n",
    "X_train_sparse = X_train[::4]\n",
    "X_test_sparse = X_test[::4]\n",
    "y_train_sparse = y_train[::4]\n",
    "y_test_sparse = y_test[::4]\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
    "X_test_scaled = scaler.transform(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7380fe6d-ddb9-4bf3-9ffe-31ba43fa8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logit Function\n",
    "def logit(x):\n",
    "    \"\"\"\n",
    "    Applies the logit function element-wise to the input array.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: ndarray\n",
    "    \n",
    "    Returns:\n",
    "    - logit: ndarray\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define Prediction Function using Logit\n",
    "def predict_probs_logit(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models with logit activation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "    - gp_models: list of trained GP models\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    logits = np.zeros((n_test, num_classes))\n",
    "\n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior[\"f(x)\"]  # Extract mean predictions\n",
    "        logits[:, class_label] = mean.flatten()\n",
    "\n",
    "    # Apply logit to convert logits to probabilities\n",
    "    probabilities = logit(logits)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233ca6ba-9ad2-4b03-9a50-7854ffc56219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Probit Function with Variance Incorporation\n",
    "def probit_with_variance(mu, sigma2):\n",
    "    \"\"\"\n",
    "    Applies the probit function with variance adjustment.\n",
    "    \n",
    "    Parameters:\n",
    "    - mu: ndarray of shape (n_test, num_classes)\n",
    "      Posterior means for each class.\n",
    "    - sigma2: ndarray of shape (n_test, num_classes)\n",
    "      Posterior variances for each class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: ndarray of shape (n_test, num_classes)\n",
    "      Adjusted probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Adjust the mean by incorporating variance\n",
    "    adjusted_mu = mu / np.sqrt(1 + sigma2)\n",
    "    return norm.cdf(adjusted_mu)\n",
    "\n",
    "# Define Prediction Function using Probit Link with Variance\n",
    "def predict_probs_probit_with_variance(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models\n",
    "    with probit activation, incorporating posterior variance.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "      Test data features.\n",
    "    - gp_models: list of trained GP models\n",
    "      One GP model per class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "      Class probabilities for each test sample.\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store means and variances\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "    variances = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior_mean = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior_mean[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "        \n",
    "        # Compute the posterior variance for the test data\n",
    "        posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "        variance = posterior_cov[\"v(x)\"]  # Extract variances\n",
    "        variances[:, class_label] = variance.flatten()\n",
    "    \n",
    "    # Apply probit with variance to convert means and variances to probabilities\n",
    "    probabilities = probit_with_variance(means, variances)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4303d1a0-efc0-44d7-a307-ed354ee1fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the Sliced Wasserstein Exponential Kernel\n",
    "def sliced_wasserstein_exponential_kernel_directions(X1, X2, hyperparameters, directions):\n",
    "    \"\"\"\n",
    "    Computes the sliced Wasserstein exponential kernel using PCA directions for projections.\n",
    "\n",
    "    Parameters:\n",
    "    - X1: (n1, d) ndarray\n",
    "    - X2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "    - directions: (k, d) ndarray, selected directions\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    length_scale = hyperparameters[0]\n",
    "    \n",
    "    n1, d = X1.shape\n",
    "    n2, _ = X2.shape\n",
    "    k = directions.shape[0]  \n",
    "    \n",
    "    # Initialize the kernel matrix\n",
    "    kernel_matrix = np.zeros((n1, n2))\n",
    "    \n",
    "    # Iterate over each PCA direction\n",
    "    for dir_idx in range(k):\n",
    "        direction = directions[dir_idx]\n",
    "        \n",
    "        # Project the data onto the current PCA direction\n",
    "        proj_X1 = X1.dot(direction)  # Shape: (n1,)\n",
    "        proj_X2 = X2.dot(direction)  # Shape: (n2,)\n",
    "        \n",
    "        # Compute the absolute differences between projections\n",
    "        abs_diff = np.abs(proj_X1[:, np.newaxis] - proj_X2[np.newaxis, :])  # Shape: (n1, n2)\n",
    "        \n",
    "        # Apply the exponential kernel\n",
    "        kernel_matrix += exponential_kernel(abs_diff, length_scale)\n",
    "    \n",
    "    # Average over all directions\n",
    "    kernel_matrix /= k\n",
    "    \n",
    "    # Add jitter for numerical stability\n",
    "    jitter = 1e-3\n",
    "    if X1.shape[0] == X2.shape[0]:\n",
    "        kernel_matrix += jitter * np.eye(X1.shape[0])\n",
    "    \n",
    "    return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e40c17-4bf6-4b70-9217-db28fb1cf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Hyperparameters and Bounds\n",
    "initial_length_scale = 1.0  # Initial guess for length scale\n",
    "init_hyperparameters = np.array([initial_length_scale])\n",
    "\n",
    "# Define bounds for the length scale (e.g., between 0.1 and 10)\n",
    "length_scale_bounds = np.array([[0.1, 10.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e4dc67-ae9a-4b93-b39c-af4372ebed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "optimal_num_components = np.argmax(cumulative_variance >= 0.95) + 1  # +1 because indices start at 0\n",
    "selected_directions_pca = pca.components_[:optimal_num_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fea6320-7e1f-40da-a750-fedf8146fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the PCA directions are computed beforehand and stored in `selected_directions`\n",
    "def sliced_wasserstein_exponential_kernel_wrapper_pca(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    # Use the previously computed PCA directions\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09fe8d14-30cc-446b-88c3-b43a6f6efe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy\n",
    "gp_models_pca = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_pca,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=200,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "\n",
    "    gp_models_pca.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "887d7434-42ab-4b6f-a4a7-60a309210e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA W-2 Logit Link – Accuracy: 0.9333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.75      1.00      0.86         3\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       0.80      1.00      0.89         4\n",
      "           7       0.75      1.00      0.86         3\n",
      "           8       1.00      0.86      0.92         7\n",
      "           9       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.95      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Logit Link Function\n",
    "gp_probabilities_logit_pca = predict_probs_logit(X_test_scaled, gp_models_pca)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_logit_pca = np.argmax(gp_probabilities_logit_pca, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_logit_pca = accuracy_score(y_test_sparse, gp_predictions_logit_pca)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'PCA W-2 Logit Link – Accuracy: {gp_accuracy_logit_pca:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_logit_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "201f2037-cc5a-4763-9b43-ef6fba2f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA W-2 Probit Link – Accuracy: 0.9333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.75      1.00      0.86         3\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      1.00      1.00         4\n",
      "           5       1.00      1.00      1.00         4\n",
      "           6       0.80      1.00      0.89         4\n",
      "           7       0.75      1.00      0.86         3\n",
      "           8       1.00      0.86      0.92         7\n",
      "           9       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.95      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_pca = predict_probs_probit_with_variance(X_test_scaled, gp_models_pca)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_pca = np.argmax(gp_probabilities_probit_pca, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_probit_pca = accuracy_score(y_test_sparse, gp_predictions_probit_pca)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'PCA W-2 Probit Link – Accuracy: {gp_accuracy_probit_pca:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e95bd8-0f8f-44ce-b954-cdeebde74c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
