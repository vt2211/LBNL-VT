{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1085846e-2b28-4e17-aae6-d1ede099230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from fvgp import GP\n",
    "from fvgp.gp_kernels import exponential_kernel\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6017db6-0786-4375-a6db-7176a55839a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess the Digits Dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3863ac-d3e7-44ad-8975-b5ae8c474737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Sparse Datasets by Selecting Every 5th Sample\n",
    "X_train_sparse = X_train[::5]\n",
    "X_test_sparse = X_test[::5]\n",
    "y_train_sparse = y_train[::5]\n",
    "y_test_sparse = y_test[::5]\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
    "X_test_scaled = scaler.transform(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7380fe6d-ddb9-4bf3-9ffe-31ba43fa8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sigmoid (Logit) Function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid function element-wise to the input array.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: ndarray\n",
    "    \n",
    "    Returns:\n",
    "    - sigmoided: ndarray\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define Prediction Function using Sigmoid (Logit)\n",
    "def predict_probs_sigmoid(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models with sigmoid activation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "    - gp_models: list of trained GP models\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    logits = np.zeros((n_test, num_classes))\n",
    "\n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior[\"f(x)\"]  # Extract mean predictions\n",
    "        logits[:, class_label] = mean.flatten()\n",
    "\n",
    "    # Apply sigmoid to convert logits to probabilities\n",
    "    probabilities = sigmoid(logits)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233ca6ba-9ad2-4b03-9a50-7854ffc56219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Probit Function with Variance Incorporation\n",
    "def probit_with_variance(mu, sigma2):\n",
    "    \"\"\"\n",
    "    Applies the probit function with variance adjustment.\n",
    "    \n",
    "    Parameters:\n",
    "    - mu: ndarray of shape (n_test, num_classes)\n",
    "      Posterior means for each class.\n",
    "    - sigma2: ndarray of shape (n_test, num_classes)\n",
    "      Posterior variances for each class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: ndarray of shape (n_test, num_classes)\n",
    "      Adjusted probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Adjust the mean by incorporating variance\n",
    "    adjusted_mu = mu / np.sqrt(1 + sigma2)\n",
    "    return norm.cdf(adjusted_mu)\n",
    "\n",
    "# Define Prediction Function using Probit Link with Variance\n",
    "def predict_probs_probit_with_variance(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models\n",
    "    with probit activation, incorporating posterior variance.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "      Test data features.\n",
    "    - gp_models: list of trained GP models\n",
    "      One GP model per class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "      Class probabilities for each test sample.\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store means and variances\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "    variances = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior_mean = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior_mean[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "        \n",
    "        # Compute the posterior variance for the test data\n",
    "        posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "        variance = posterior_cov[\"v(x)\"]  # Extract variances\n",
    "        variances[:, class_label] = variance.flatten()\n",
    "    \n",
    "    # Apply probit with variance to convert means and variances to probabilities\n",
    "    probabilities = probit_with_variance(means, variances)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4303d1a0-efc0-44d7-a307-ed354ee1fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the Sliced Wasserstein Exponential Kernel\n",
    "def sliced_wasserstein_exponential_kernel_directions(X1, X2, hyperparameters, directions):\n",
    "    \"\"\"\n",
    "    Computes the sliced Wasserstein exponential kernel using PCA directions for projections.\n",
    "\n",
    "    Parameters:\n",
    "    - X1: (n1, d) ndarray\n",
    "    - X2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "    - directions: (k, d) ndarray, selected directions\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    length_scale = hyperparameters[0]\n",
    "    \n",
    "    n1, d = X1.shape\n",
    "    n2, _ = X2.shape\n",
    "    k = directions.shape[0]  \n",
    "    \n",
    "    # Initialize the kernel matrix\n",
    "    kernel_matrix = np.zeros((n1, n2))\n",
    "    \n",
    "    # Iterate over each PCA direction\n",
    "    for dir_idx in range(k):\n",
    "        direction = directions[dir_idx]\n",
    "        \n",
    "        # Project the data onto the current PCA direction\n",
    "        proj_X1 = X1.dot(direction)  # Shape: (n1,)\n",
    "        proj_X2 = X2.dot(direction)  # Shape: (n2,)\n",
    "        \n",
    "        # Compute the absolute differences between projections\n",
    "        abs_diff = np.abs(proj_X1[:, np.newaxis] - proj_X2[np.newaxis, :])  # Shape: (n1, n2)\n",
    "        \n",
    "        # Apply the exponential kernel\n",
    "        kernel_matrix += exponential_kernel(abs_diff, length_scale)\n",
    "    \n",
    "    # Average over all directions\n",
    "    kernel_matrix /= k\n",
    "    \n",
    "    # Add jitter for numerical stability\n",
    "    jitter = 1e-3\n",
    "    if X1.shape[0] == X2.shape[0]:\n",
    "        kernel_matrix += jitter * np.eye(X1.shape[0])\n",
    "    \n",
    "    return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3e40c17-4bf6-4b70-9217-db28fb1cf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Hyperparameters and Bounds\n",
    "initial_length_scale = 1.0  # Initial guess for length scale\n",
    "init_hyperparameters = np.array([initial_length_scale])\n",
    "\n",
    "# Define bounds for the length scale (e.g., between 0.1 and 10)\n",
    "length_scale_bounds = np.array([[0.1, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3a332-17f7-4b48-b268-78d04f9b51ab",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27e4dc67-ae9a-4b93-b39c-af4372ebed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "optimal_num_components = np.argmax(cumulative_variance >= 0.95) + 1  # +1 because indices start at 0\n",
    "selected_directions_pca = pca.components_[:optimal_num_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fea6320-7e1f-40da-a750-fedf8146fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the PCA directions are computed beforehand and stored in `selected_directions`\n",
    "def sliced_wasserstein_exponential_kernel_wrapper_pca(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    # Use the previously computed PCA directions\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09fe8d14-30cc-446b-88c3-b43a6f6efe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy\n",
    "gp_models_pca = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_pca,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "\n",
    "    gp_models_pca.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "887d7434-42ab-4b6f-a4a7-60a309210e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA W-2 Sigmoid Link – Accuracy: 0.7778\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.60      0.60      0.60         5\n",
      "           2       0.40      0.67      0.50         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.33      0.50         3\n",
      "           5       0.75      1.00      0.86         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.83      0.71      0.77         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.84      0.80      0.79        36\n",
      "weighted avg       0.82      0.78      0.78        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid_pca = predict_probs_sigmoid(X_test_scaled, gp_models_pca)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid_pca = np.argmax(gp_probabilities_sigmoid_pca, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid_pca = accuracy_score(y_test_sparse, gp_predictions_sigmoid_pca)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'PCA W-2 Sigmoid Link – Accuracy: {gp_accuracy_sigmoid_pca:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "201f2037-cc5a-4763-9b43-ef6fba2f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fvgp/gp_posterior.py:109: UserWarning: Negative variances encountered. That normally means that the model is unstable. Rethink the kernel definition, add more noise to the data, or double check the hyperparameter optimization bounds. This will not terminate the algorithm, but expect anomalies.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA W-2 Probit Link – Accuracy: 0.9167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.60      0.60      0.60         5\n",
      "           2       0.40      0.67      0.50         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.33      0.50         3\n",
      "           5       0.75      1.00      0.86         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.83      0.71      0.77         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.84      0.80      0.79        36\n",
      "weighted avg       0.82      0.78      0.78        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_pca = predict_probs_probit_with_variance(X_test_scaled, gp_models_pca)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_pca = np.argmax(gp_probabilities_probit_pca, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "gp_accuracy_probit_pca = accuracy_score(y_test_sparse, gp_predictions_probit_pca)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'PCA W-2 Probit Link – Accuracy: {gp_accuracy_probit_var:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0d0bf-2500-4892-82ce-96688f0b82ca",
   "metadata": {},
   "source": [
    "### Uniform Directions using Fibonacci Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffdf84a2-7800-4569-b506-4398b1557252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_directions(d, n_directions):\n",
    "    \"\"\"\n",
    "    Generates uniformly distributed directions on the unit sphere in d dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - d: int, dimensionality of the data.\n",
    "    - n_directions: int, number of directions to generate.\n",
    "\n",
    "    Returns:\n",
    "    - directions: (n_directions, d) ndarray, each row is a unit vector.\n",
    "    \"\"\"\n",
    "    if d == 2:\n",
    "        # Uniformly spaced angles around the circle\n",
    "        angles = np.linspace(0, 2 * np.pi, n_directions, endpoint=False)\n",
    "        directions = np.stack([np.cos(angles), np.sin(angles)], axis=1)\n",
    "    elif d == 3:\n",
    "        # Fibonacci lattice for uniform points on a sphere\n",
    "        indices = np.arange(0, n_directions, dtype=float) + 0.5\n",
    "        phi = np.arccos(1 - 2*indices/n_directions)\n",
    "        theta = np.pi * (1 + 5**0.5) * indices\n",
    "        x, y_dir, z = (np.cos(theta) * np.sin(phi),\n",
    "                      np.sin(theta) * np.sin(phi),\n",
    "                      np.cos(phi))\n",
    "        directions = np.stack([x, y_dir, z], axis=1)\n",
    "    else:\n",
    "        # For higher dimensions, generate random directions and normalize\n",
    "        directions = np.random.randn(n_directions, d)\n",
    "        directions /= np.linalg.norm(directions, axis=1, keepdims=True)\n",
    "    return directions\n",
    "\n",
    "d = X_train_scaled.shape[1]  # Dimensionality of the data\n",
    "selected_directions_uniform = generate_uniform_directions(d, n_directions=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8f4394c-d209-4291-ad24-dfadbe46e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliced_wasserstein_exponential_kernel_wrapper_uniform(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel using uniform grid directions.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d10ffd5-ef8e-4d98-b618-9ab185488325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy\n",
    "gp_models_uniform = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_uniform,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "\n",
    "    gp_models_uniform.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da827108-903e-4962-ad53-a10f68fad177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform W-2 Sigmoid Link – Accuracy: 0.8889\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       0.60      1.00      0.75         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       1.00      0.71      0.83         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.89        36\n",
      "   macro avg       0.93      0.90      0.90        36\n",
      "weighted avg       0.93      0.89      0.89        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid_uniform = predict_probs_sigmoid(X_test_scaled, gp_models_uniform)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid_uniform = np.argmax(gp_probabilities_sigmoid_uniform, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid_uniform = accuracy_score(y_test_sparse, gp_predictions_sigmoid_uniform)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Uniform W-2 Sigmoid Link – Accuracy: {gp_accuracy_sigmoid_uniform:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid_uniform))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43367960-1299-4d28-8f4b-3238d61542a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform W-2 Probit Link – Accuracy: 0.8889\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       0.60      1.00      0.75         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       1.00      0.71      0.83         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.89        36\n",
      "   macro avg       0.93      0.90      0.90        36\n",
      "weighted avg       0.93      0.89      0.89        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_uniform = predict_probs_probit_with_variance(X_test_scaled, gp_models_uniform)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_uniform = np.argmax(gp_probabilities_probit_uniform, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "gp_accuracy_probit_uniform = accuracy_score(y_test_sparse, gp_predictions_probit_uniform)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Uniform W-2 Probit Link – Accuracy: {gp_accuracy_probit_uniform:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_uniform))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c61fc0-87d5-4840-84c6-1e8bb9dd4c0e",
   "metadata": {},
   "source": [
    "### Orthogonal Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5590197b-eee6-4f63-b3f5-a0e364a04af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orthogonal_directions(d, k):\n",
    "    \"\"\"\n",
    "    Generates k mutually orthogonal unit vectors in d-dimensional space using QR decomposition.\n",
    "\n",
    "    Parameters:\n",
    "    - d: int, dimensionality of the data.\n",
    "    - k: int, number of orthogonal directions to generate (k <= d).\n",
    "\n",
    "    Returns:\n",
    "    - directions: (k, d) ndarray, each row is a unit vector.\n",
    "    \"\"\"\n",
    "    if k > d:\n",
    "        raise ValueError(\"Number of directions k cannot exceed dimensionality d.\")\n",
    "    \n",
    "    # Step 1: Generate a random d x k matrix\n",
    "    random_matrix = np.random.randn(d, k)\n",
    "    \n",
    "    # Step 2: Perform QR decomposition\n",
    "    Q, R = np.linalg.qr(random_matrix)\n",
    "    \n",
    "    # Step 3: Extract the first k columns as orthogonal directions\n",
    "    directions = Q[:, :k].T  # Shape: (k, d)\n",
    "    \n",
    "    return directions\n",
    "\n",
    "d = X_train_scaled.shape[1]\n",
    "k = 16  # Number of orthogonal directions (adjust as needed)\n",
    "selected_directions_orthogonal = generate_orthogonal_directions(d, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5859eb8-bef1-4d72-9f75-1a757d7b4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliced_wasserstein_exponential_kernel_wrapper_orthogonal(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel using orthogonal directions.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_orthogonal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8bcb184-7c11-44d5-9344-ce5744a302d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy with Orthogonal Directions\n",
    "gp_models_orthogonal = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "    \n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "    \n",
    "    # Initialize GP model with the orthogonal directions-based kernel\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_orthogonal,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "    \n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "    \n",
    "    gp_models_orthogonal.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9853b712-b586-4c6a-b7dc-21f8bec58f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal W-2 Sigmoid Link – Accuracy: 0.7778\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.50      0.80      0.62         5\n",
      "           2       0.40      0.67      0.50         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       0.83      1.00      0.91         5\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       1.00      0.29      0.44         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.87      0.84      0.83        36\n",
      "weighted avg       0.86      0.78      0.77        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid_orthogonal = predict_probs_sigmoid(X_test_scaled, gp_models_orthogonal)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid_orthogonal = np.argmax(gp_probabilities_sigmoid_orthogonal, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid_orthogonal = accuracy_score(y_test_sparse, gp_predictions_sigmoid_orthogonal)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Orthogonal W-2 Sigmoid Link – Accuracy: {gp_accuracy_sigmoid_orthogonal:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid_orthogonal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2215f650-4083-4b9d-afa6-37d697e1e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal W-2 Probit Link – Accuracy: 0.7778\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.50      0.80      0.62         5\n",
      "           2       0.40      0.67      0.50         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       0.83      1.00      0.91         5\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       1.00      0.29      0.44         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.87      0.84      0.83        36\n",
      "weighted avg       0.86      0.78      0.77        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_orthogonal = predict_probs_probit_with_variance(X_test_scaled, gp_models_orthogonal)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_orthogonal = np.argmax(gp_probabilities_probit_orthogonal, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "gp_accuracy_probit_orthogonal = accuracy_score(y_test_sparse, gp_predictions_probit_orthogonal)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Orthogonal W-2 Probit Link – Accuracy: {gp_accuracy_probit_orthogonal:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_orthogonal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949abce-3af6-465d-9e6d-6bbedbd0a0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
