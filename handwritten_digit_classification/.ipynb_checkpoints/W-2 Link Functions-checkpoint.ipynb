{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1085846e-2b28-4e17-aae6-d1ede099230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from fvgp import GP\n",
    "from fvgp.gp_kernels import exponential_kernel\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6017db6-0786-4375-a6db-7176a55839a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess the Digits Dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3863ac-d3e7-44ad-8975-b5ae8c474737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Sparse Datasets by Selecting Every 5th Sample\n",
    "X_train_sparse = X_train[::5]\n",
    "X_test_sparse = X_test[::5]\n",
    "y_train_sparse = y_train[::5]\n",
    "y_test_sparse = y_test[::5]\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
    "X_test_scaled = scaler.transform(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b05ae7-2ef7-4f2d-b7c5-67ed1da0609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the Sliced Wasserstein Exponential Kernel\n",
    "def sliced_wasserstein_exponential_kernel(X1, X2, hyperparameters, n_directions=64):\n",
    "    \"\"\"\n",
    "    Computes the sliced Wasserstein exponential kernel between two datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - X1: (n1, d) ndarray\n",
    "    - X2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "    - n_directions: int, number of random projections\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    length_scale = hyperparameters[0]\n",
    "    \n",
    "    n1, d = X1.shape\n",
    "    n2, _ = X2.shape\n",
    "\n",
    "    # Generate random directions (unit vectors)\n",
    "    directions = np.random.randn(n_directions, d)\n",
    "    directions /= np.linalg.norm(directions, axis=1, keepdims=True)  # Normalize to unit vectors\n",
    "\n",
    "    # Initialize kernel matrix\n",
    "    kernel_matrix = np.zeros((n1, n2))\n",
    "\n",
    "    # Iterate over each direction\n",
    "    for dir_idx in range(n_directions):\n",
    "        direction = directions[dir_idx]\n",
    "\n",
    "        # Project the data onto the current direction\n",
    "        proj_X1 = X1.dot(direction)  # Shape: (n1,)\n",
    "        proj_X2 = X2.dot(direction)  # Shape: (n2,)\n",
    "\n",
    "        # Compute the absolute differences between projections\n",
    "        # Broadcasting to compute pairwise |x_i - y_j|\n",
    "        abs_diff = np.abs(proj_X1[:, np.newaxis] - proj_X2[np.newaxis, :])  # Shape: (n1, n2)\n",
    "\n",
    "        # Apply the exponential kernel\n",
    "        kernel_matrix += exponential_kernel(abs_diff, length_scale)\n",
    "\n",
    "    # Average over all directions\n",
    "    kernel_matrix /= n_directions\n",
    "\n",
    "    # Add jitter for numerical stability\n",
    "    jitter = 1e-3\n",
    "    if X1.shape[0] == X2.shape[0]:\n",
    "        kernel_matrix += jitter * np.eye(X1.shape[0])\n",
    "\n",
    "    return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fea6320-7e1f-40da-a750-fedf8146fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliced_wasserstein_exponential_kernel_wrapper(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    return sliced_wasserstein_exponential_kernel(x1, x2, hyperparameters, n_directions=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa90866-e4c8-4966-a8a1-d13905f51bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Hyperparameters and Bounds\n",
    "initial_length_scale = 1.0  # Initial guess for length scale\n",
    "init_hyperparameters = np.array([initial_length_scale])\n",
    "\n",
    "# Define bounds for the length scale (e.g., between 0.1 and 10)\n",
    "length_scale_bounds = np.array([[0.1, 10.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09fe8d14-30cc-446b-88c3-b43a6f6efe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy\n",
    "gp_models_wasserstein = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "\n",
    "    gp_models_wasserstein.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7773bae8-775c-4cb0-b0c1-9b402e7ee4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define Prediction Function\n",
    "def predict_probs(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models.\n",
    "\n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "    - gp_models: list of trained GP models\n",
    "\n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "\n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "\n",
    "    # Apply softmax to convert means to probabilities\n",
    "    probabilities = softmax(means)\n",
    "    return probabilities\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Applies the softmax function to each row of the input array.\n",
    "\n",
    "    Parameters:\n",
    "    - x: (n, num_classes) ndarray\n",
    "\n",
    "    Returns:\n",
    "    - softmaxed: (n, num_classes) ndarray\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc148853-4c5f-4029-a610-4477758810d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Make Predictions on the Test Set\n",
    "gp_probabilities_wasserstein = predict_probs(X_test_scaled, gp_models_wasserstein)\n",
    "gp_predictions_wasserstein = np.argmax(gp_probabilities_wasserstein, axis=1)\n",
    "gp_accuracy_wasserstein = accuracy_score(y_test_sparse, gp_predictions_wasserstein)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97c8f7e1-f502-4715-b324-86720916f053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Classifier with Sliced Wasserstein Kernel – Accuracy: 0.8333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73         4\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       0.83      1.00      0.91         5\n",
      "           7       0.67      0.67      0.67         3\n",
      "           8       0.80      0.57      0.67         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.83        36\n",
      "   macro avg       0.89      0.87      0.87        36\n",
      "weighted avg       0.86      0.83      0.83        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Evaluate the Classifier\n",
    "print(f'GP Classifier with Sliced Wasserstein Kernel – Accuracy: {gp_accuracy_wasserstein:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_wasserstein))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c4545-0360-4fa1-9b10-44781b9bcc56",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "- include the multiplication kernel\n",
    "- standardize the projected directions\n",
    "- kernel choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7e181-0320-4b1b-be5f-cd1da2816c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "887d7434-42ab-4b6f-a4a7-60a309210e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Classifier with Sigmoid Link Function – Accuracy: 0.8611\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      0.60      0.75         5\n",
      "           2       0.75      1.00      0.86         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       0.50      1.00      0.67         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       1.00      0.71      0.83         7\n",
      "           9       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.88      0.90      0.86        36\n",
      "weighted avg       0.92      0.86      0.87        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the Sigmoid (Logit) Function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid function element-wise to the input array.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: ndarray\n",
    "    \n",
    "    Returns:\n",
    "    - sigmoided: ndarray\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define Prediction Function using Sigmoid (Logit)\n",
    "def predict_probs_sigmoid(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models with sigmoid activation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "    - gp_models: list of trained GP models\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    logits = np.zeros((n_test, num_classes))\n",
    "\n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior[\"f(x)\"]  # Extract mean predictions\n",
    "        logits[:, class_label] = mean.flatten()\n",
    "\n",
    "    # Apply sigmoid to convert logits to probabilities\n",
    "    probabilities = sigmoid(logits)\n",
    "    return probabilities\n",
    "\n",
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid = predict_probs_sigmoid(X_test_scaled, gp_models_wasserstein)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid = np.argmax(gp_probabilities_sigmoid, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid = accuracy_score(y_test_sparse, gp_predictions_sigmoid)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'GP Classifier with Sigmoid Link Function – Accuracy: {gp_accuracy_sigmoid:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5efe710e-3edf-4d4d-b716-4b1a0da0c6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Classifier with Probit Link Function – Accuracy: 0.8333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.80      0.80      0.80         5\n",
      "           2       0.60      1.00      0.75         3\n",
      "           3       1.00      0.50      0.67         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       0.83      1.00      0.91         5\n",
      "           7       0.50      0.67      0.57         3\n",
      "           8       1.00      0.57      0.73         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.83        36\n",
      "   macro avg       0.87      0.85      0.84        36\n",
      "weighted avg       0.87      0.83      0.83        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define the Probit Function\n",
    "def probit(x):\n",
    "    \"\"\"\n",
    "    Applies the probit (Gaussian CDF) function element-wise to the input array.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: ndarray\n",
    "    \n",
    "    Returns:\n",
    "    - probited: ndarray\n",
    "    \"\"\"\n",
    "    return norm.cdf(x)\n",
    "\n",
    "# Define Prediction Function using Probit Link\n",
    "def predict_probs_probit(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models with probit activation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "    - gp_models: list of trained GP models\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    logits = np.zeros((n_test, num_classes))\n",
    "\n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior[\"f(x)\"]  # Extract mean predictions\n",
    "        logits[:, class_label] = mean.flatten()\n",
    "\n",
    "    # Apply probit (Gaussian CDF) to convert logits to probabilities\n",
    "    probabilities = probit(logits)\n",
    "    return probabilities\n",
    "\n",
    "# Predict Probabilities using Probit Link Function\n",
    "gp_probabilities_probit = predict_probs_probit(X_test_scaled, gp_models_wasserstein)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit = np.argmax(gp_probabilities_probit, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_probit = accuracy_score(y_test_sparse, gp_predictions_probit)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'GP Classifier with Probit Link Function – Accuracy: {gp_accuracy_probit:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "201f2037-cc5a-4763-9b43-ef6fba2f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/fvgp/gp_posterior.py:109: UserWarning: Negative variances encountered. That normally means that the model is unstable. Rethink the kernel definition, add more noise to the data, or double check the hyperparameter optimization bounds. This will not terminate the algorithm, but expect anomalies.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Classifier with Probit Link Function (Incorporating Variance) – Accuracy: 0.8611\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      0.60      0.75         5\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      0.50      0.67         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       0.60      1.00      0.75         3\n",
      "           6       0.83      1.00      0.91         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.86      0.86      0.86         7\n",
      "           9       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.88      0.86      0.84        36\n",
      "weighted avg       0.90      0.86      0.86        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define the Probit Function with Variance Incorporation\n",
    "def probit_with_variance(mu, sigma2):\n",
    "    \"\"\"\n",
    "    Applies the probit function with variance adjustment.\n",
    "    \n",
    "    Parameters:\n",
    "    - mu: ndarray of shape (n_test, num_classes)\n",
    "      Posterior means for each class.\n",
    "    - sigma2: ndarray of shape (n_test, num_classes)\n",
    "      Posterior variances for each class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: ndarray of shape (n_test, num_classes)\n",
    "      Adjusted probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Adjust the mean by incorporating variance\n",
    "    adjusted_mu = mu / np.sqrt(1 + sigma2)\n",
    "    return norm.cdf(adjusted_mu)\n",
    "\n",
    "# Define Prediction Function using Probit Link with Variance\n",
    "def predict_probs_probit_with_variance(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models\n",
    "    with probit activation, incorporating posterior variance.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "      Test data features.\n",
    "    - gp_models: list of trained GP models\n",
    "      One GP model per class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "      Class probabilities for each test sample.\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store means and variances\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "    variances = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior_mean = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior_mean[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "        \n",
    "        # Compute the posterior variance for the test data\n",
    "        posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "        variance = posterior_cov[\"v(x)\"]  # Extract variances\n",
    "        variances[:, class_label] = variance.flatten()\n",
    "    \n",
    "    # Apply probit with variance to convert means and variances to probabilities\n",
    "    probabilities = probit_with_variance(means, variances)\n",
    "    return probabilities\n",
    "\n",
    "# Example Usage\n",
    "# Assuming:\n",
    "# - X_test_scaled: Scaled test features\n",
    "# - gp_models_wasserstein: List of trained GP models (one per class)\n",
    "# - y_test_sparse: True class labels for test data\n",
    "\n",
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_var = predict_probs_probit_with_variance(X_test_scaled, gp_models_wasserstein)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_var = np.argmax(gp_probabilities_probit_var, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_probit_var = accuracy_score(y_test_sparse, gp_predictions_probit_var)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'GP Classifier with Probit Link Function (Incorporating Variance) – Accuracy: {gp_accuracy_probit_var:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df286674-0723-4f7d-9bb2-14c429d91f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3222a70-eb41-45ba-85d7-96b2f76b39b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d3b69-c8e7-464c-ab4b-3a3f2b3ea645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ffcb5-a40c-4d21-9771-8d674f74736a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29044c3-59bc-4377-9ad7-46259c6fb521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "45f8a2b4-98bf-45a0-9a80-4cabff748b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327be89f-028a-4980-925e-5bd4a1703b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631ed69-5dde-440a-8f33-9ce26e1c6f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d92040-f18b-4f11-8dcd-2d676c8331ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f3c43754-afc7-4a91-9225-c0e6ee9d550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "variance = posterior_cov[\"v(x)\"]  # Extract variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3cf6bf8f-dc2c-4ca2-9849-5bd445012902",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = gp_models_wasserstein[1].posterior_mean(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58128a39-4262-46e3-b608-ed572f7793d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2785934 ,  0.16631681,  0.55872668,  0.0372537 , -0.35681185,\n",
       "        0.20374535, -0.37956651,  0.40197116, -0.12222842,  0.54569658,\n",
       "        0.69614941,  0.48639725,  0.45458562, -0.20851064, -0.00963746,\n",
       "        0.03445074, -0.22813226, -0.18584793,  0.13580092,  0.11601559,\n",
       "       -0.48673687,  0.01265757, -0.03767933, -0.24778734, -0.2799685 ,\n",
       "       -0.05920403,  0.81352519,  0.05504309, -0.18069925,  0.67161847,\n",
       "       -0.32319436, -0.04650726, -0.00880823,  0.01705904, -0.1882333 ,\n",
       "       -0.53894496])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_1['f(x)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2df2879-c2a9-49af-a3cb-f086eb3e9363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.340997</td>\n",
       "      <td>-1.089832</td>\n",
       "      <td>-0.438623</td>\n",
       "      <td>-0.628338</td>\n",
       "      <td>-1.036727</td>\n",
       "      <td>-0.42081</td>\n",
       "      <td>-0.129081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.673579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780330</td>\n",
       "      <td>-0.208066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.273746</td>\n",
       "      <td>-1.083555</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.045253</td>\n",
       "      <td>-1.180209</td>\n",
       "      <td>-0.539294</td>\n",
       "      <td>-0.203638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.340997</td>\n",
       "      <td>-1.089832</td>\n",
       "      <td>-2.359080</td>\n",
       "      <td>0.925719</td>\n",
       "      <td>-0.860679</td>\n",
       "      <td>-0.42081</td>\n",
       "      <td>-0.129081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.673579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780330</td>\n",
       "      <td>-0.208066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.273746</td>\n",
       "      <td>-1.083555</td>\n",
       "      <td>-2.132532</td>\n",
       "      <td>0.827232</td>\n",
       "      <td>-0.018945</td>\n",
       "      <td>-0.539294</td>\n",
       "      <td>-0.203638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.340997</td>\n",
       "      <td>-0.428586</td>\n",
       "      <td>0.281549</td>\n",
       "      <td>0.925719</td>\n",
       "      <td>-0.156487</td>\n",
       "      <td>-0.42081</td>\n",
       "      <td>-0.129081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.337845</td>\n",
       "      <td>...</td>\n",
       "      <td>1.993634</td>\n",
       "      <td>-0.208066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.273746</td>\n",
       "      <td>-0.066545</td>\n",
       "      <td>0.239388</td>\n",
       "      <td>0.436243</td>\n",
       "      <td>0.146950</td>\n",
       "      <td>-0.080682</td>\n",
       "      <td>-0.203638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.340997</td>\n",
       "      <td>-1.089832</td>\n",
       "      <td>0.521606</td>\n",
       "      <td>0.481703</td>\n",
       "      <td>0.547705</td>\n",
       "      <td>-0.42081</td>\n",
       "      <td>-0.129081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.673579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780330</td>\n",
       "      <td>-0.208066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.273746</td>\n",
       "      <td>-0.473349</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.045253</td>\n",
       "      <td>-1.180209</td>\n",
       "      <td>-0.539294</td>\n",
       "      <td>-0.203638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.340997</td>\n",
       "      <td>0.893907</td>\n",
       "      <td>0.281549</td>\n",
       "      <td>0.925719</td>\n",
       "      <td>-0.156487</td>\n",
       "      <td>-0.42081</td>\n",
       "      <td>-0.129081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.332133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804792</td>\n",
       "      <td>-0.208066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.273746</td>\n",
       "      <td>0.543661</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.240748</td>\n",
       "      <td>1.308215</td>\n",
       "      <td>-0.080682</td>\n",
       "      <td>-0.203638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5        6         7   \\\n",
       "0  0.0 -0.340997 -1.089832 -0.438623 -0.628338 -1.036727 -0.42081 -0.129081   \n",
       "1  0.0 -0.340997 -1.089832 -2.359080  0.925719 -0.860679 -0.42081 -0.129081   \n",
       "2  0.0 -0.340997 -0.428586  0.281549  0.925719 -0.156487 -0.42081 -0.129081   \n",
       "3  0.0 -0.340997 -1.089832  0.521606  0.481703  0.547705 -0.42081 -0.129081   \n",
       "4  0.0 -0.340997  0.893907  0.281549  0.925719 -0.156487 -0.42081 -0.129081   \n",
       "\n",
       "    8         9   ...        54        55   56        57        58        59  \\\n",
       "0  0.0 -0.673579  ... -0.780330 -0.208066  0.0 -0.273746 -1.083555  0.002196   \n",
       "1  0.0 -0.673579  ... -0.780330 -0.208066  0.0 -0.273746 -1.083555 -2.132532   \n",
       "2  0.0  1.337845  ...  1.993634 -0.208066  0.0 -0.273746 -0.066545  0.239388   \n",
       "3  0.0 -0.673579  ... -0.780330 -0.208066  0.0 -0.273746 -0.473349  0.002196   \n",
       "4  0.0  0.332133  ...  0.804792 -0.208066  0.0 -0.273746  0.543661  0.002196   \n",
       "\n",
       "         60        61        62        63  \n",
       "0  0.045253 -1.180209 -0.539294 -0.203638  \n",
       "1  0.827232 -0.018945 -0.539294 -0.203638  \n",
       "2  0.436243  0.146950 -0.080682 -0.203638  \n",
       "3  0.045253 -1.180209 -0.539294 -0.203638  \n",
       "4  0.240748  1.308215 -0.080682 -0.203638  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df = pd.DataFrame(X_train_scaled)\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1fba0973-83ea-47e9-951a-aac7d71740eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.099954</td>\n",
       "      <td>0.081411</td>\n",
       "      <td>0.067875</td>\n",
       "      <td>0.069878</td>\n",
       "      <td>0.081966</td>\n",
       "      <td>0.093119</td>\n",
       "      <td>0.080028</td>\n",
       "      <td>0.215039</td>\n",
       "      <td>0.088385</td>\n",
       "      <td>0.122346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113337</td>\n",
       "      <td>0.102024</td>\n",
       "      <td>0.084755</td>\n",
       "      <td>0.072935</td>\n",
       "      <td>0.098727</td>\n",
       "      <td>0.083464</td>\n",
       "      <td>0.089146</td>\n",
       "      <td>0.098215</td>\n",
       "      <td>0.156762</td>\n",
       "      <td>0.100635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123149</td>\n",
       "      <td>0.123141</td>\n",
       "      <td>0.107766</td>\n",
       "      <td>0.071870</td>\n",
       "      <td>0.094234</td>\n",
       "      <td>0.081542</td>\n",
       "      <td>0.106764</td>\n",
       "      <td>0.112336</td>\n",
       "      <td>0.083581</td>\n",
       "      <td>0.095617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.078787</td>\n",
       "      <td>0.079488</td>\n",
       "      <td>0.103196</td>\n",
       "      <td>0.077625</td>\n",
       "      <td>0.097606</td>\n",
       "      <td>0.092306</td>\n",
       "      <td>0.214351</td>\n",
       "      <td>0.092793</td>\n",
       "      <td>0.079979</td>\n",
       "      <td>0.083868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.204036</td>\n",
       "      <td>0.064750</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>0.075201</td>\n",
       "      <td>0.090230</td>\n",
       "      <td>0.080056</td>\n",
       "      <td>0.114029</td>\n",
       "      <td>0.104445</td>\n",
       "      <td>0.088598</td>\n",
       "      <td>0.097770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.099954  0.081411  0.067875  0.069878  0.081966  0.093119  0.080028   \n",
       "1  0.113337  0.102024  0.084755  0.072935  0.098727  0.083464  0.089146   \n",
       "2  0.123149  0.123141  0.107766  0.071870  0.094234  0.081542  0.106764   \n",
       "3  0.078787  0.079488  0.103196  0.077625  0.097606  0.092306  0.214351   \n",
       "4  0.204036  0.064750  0.080884  0.075201  0.090230  0.080056  0.114029   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.215039  0.088385  0.122346  \n",
       "1  0.098215  0.156762  0.100635  \n",
       "2  0.112336  0.083581  0.095617  \n",
       "3  0.092793  0.079979  0.083868  \n",
       "4  0.104445  0.088598  0.097770  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df = pd.DataFrame(gp_probabilities_wasserstein)\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bed90b64-1ff4-44ab-979e-045108b8aac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.215039\n",
       "1    0.156762\n",
       "2    0.123149\n",
       "3    0.214351\n",
       "4    0.204036\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df.max(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ebcb0e53-67bc-4f78-a8c4-fbcc5354e9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.000000\n",
       "1     1.003096\n",
       "2     1.003096\n",
       "3     1.003096\n",
       "4     1.003096\n",
       "        ...   \n",
       "59    1.003096\n",
       "60    1.003096\n",
       "61    1.003096\n",
       "62    1.003096\n",
       "63    1.003096\n",
       "Length: 64, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df.var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47f6e5-2733-4a35-9e8b-c89083d0a11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
