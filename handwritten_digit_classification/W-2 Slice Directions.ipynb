{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1085846e-2b28-4e17-aae6-d1ede099230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from fvgp import GP\n",
    "from fvgp.gp_kernels import exponential_kernel\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6017db6-0786-4375-a6db-7176a55839a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess the Digits Dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3863ac-d3e7-44ad-8975-b5ae8c474737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Sparse Datasets by Selecting Every 5th Sample\n",
    "X_train_sparse = X_train[::5]\n",
    "X_test_sparse = X_test[::5]\n",
    "y_train_sparse = y_train[::5]\n",
    "y_test_sparse = y_test[::5]\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sparse)\n",
    "X_test_scaled = scaler.transform(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7380fe6d-ddb9-4bf3-9ffe-31ba43fa8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sigmoid (Logit) Function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid function element-wise to the input array.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: ndarray\n",
    "    \n",
    "    Returns:\n",
    "    - sigmoided: ndarray\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define Prediction Function using Sigmoid (Logit)\n",
    "def predict_probs_sigmoid(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models with sigmoid activation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "    - gp_models: list of trained GP models\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    logits = np.zeros((n_test, num_classes))\n",
    "\n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior[\"f(x)\"]  # Extract mean predictions\n",
    "        logits[:, class_label] = mean.flatten()\n",
    "\n",
    "    # Apply sigmoid to convert logits to probabilities\n",
    "    probabilities = sigmoid(logits)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233ca6ba-9ad2-4b03-9a50-7854ffc56219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Probit Function with Variance Incorporation\n",
    "def probit_with_variance(mu, sigma2):\n",
    "    \"\"\"\n",
    "    Applies the probit function with variance adjustment.\n",
    "    \n",
    "    Parameters:\n",
    "    - mu: ndarray of shape (n_test, num_classes)\n",
    "      Posterior means for each class.\n",
    "    - sigma2: ndarray of shape (n_test, num_classes)\n",
    "      Posterior variances for each class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: ndarray of shape (n_test, num_classes)\n",
    "      Adjusted probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Adjust the mean by incorporating variance\n",
    "    adjusted_mu = mu / np.sqrt(1 + sigma2)\n",
    "    return norm.cdf(adjusted_mu)\n",
    "\n",
    "# Define Prediction Function using Probit Link with Variance\n",
    "def predict_probs_probit_with_variance(X_test, gp_models):\n",
    "    \"\"\"\n",
    "    Predicts class probabilities for the test set using trained GP models\n",
    "    with probit activation, incorporating posterior variance.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_test: (n_test, d) ndarray\n",
    "      Test data features.\n",
    "    - gp_models: list of trained GP models\n",
    "      One GP model per class.\n",
    "      \n",
    "    Returns:\n",
    "    - probabilities: (n_test, num_classes) ndarray\n",
    "      Class probabilities for each test sample.\n",
    "    \"\"\"\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store means and variances\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "    variances = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior_mean = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior_mean[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "        \n",
    "        # Compute the posterior variance for the test data\n",
    "        posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "        variance = posterior_cov[\"v(x)\"]  # Extract variances\n",
    "        variances[:, class_label] = variance.flatten()\n",
    "    \n",
    "    # Apply probit with variance to convert means and variances to probabilities\n",
    "    probabilities = probit_with_variance(means, variances)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4303d1a0-efc0-44d7-a307-ed354ee1fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the Sliced Wasserstein Exponential Kernel\n",
    "def sliced_wasserstein_exponential_kernel_directions(X1, X2, hyperparameters, directions):\n",
    "    \"\"\"\n",
    "    Computes the sliced Wasserstein exponential kernel using PCA directions for projections.\n",
    "\n",
    "    Parameters:\n",
    "    - X1: (n1, d) ndarray\n",
    "    - X2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "    - directions: (k, d) ndarray, selected directions\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    length_scale = hyperparameters[0]\n",
    "    \n",
    "    n1, d = X1.shape\n",
    "    n2, _ = X2.shape\n",
    "    k = directions.shape[0]  \n",
    "    \n",
    "    # Initialize the kernel matrix\n",
    "    kernel_matrix = np.zeros((n1, n2))\n",
    "    \n",
    "    # Iterate over each PCA direction\n",
    "    for dir_idx in range(k):\n",
    "        direction = directions[dir_idx]\n",
    "        \n",
    "        # Project the data onto the current PCA direction\n",
    "        proj_X1 = X1.dot(direction)  # Shape: (n1,)\n",
    "        proj_X2 = X2.dot(direction)  # Shape: (n2,)\n",
    "        \n",
    "        # Compute the absolute differences between projections\n",
    "        abs_diff = np.abs(proj_X1[:, np.newaxis] - proj_X2[np.newaxis, :])  # Shape: (n1, n2)\n",
    "        \n",
    "        # Apply the exponential kernel\n",
    "        kernel_matrix += exponential_kernel(abs_diff, length_scale)\n",
    "    \n",
    "    # Average over all directions\n",
    "    kernel_matrix /= k\n",
    "    \n",
    "    # Add jitter for numerical stability\n",
    "    jitter = 1e-3\n",
    "    if X1.shape[0] == X2.shape[0]:\n",
    "        kernel_matrix += jitter * np.eye(X1.shape[0])\n",
    "    \n",
    "    return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3e40c17-4bf6-4b70-9217-db28fb1cf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Initialize Hyperparameters and Bounds\n",
    "initial_length_scale = 1.0  # Initial guess for length scale\n",
    "init_hyperparameters = np.array([initial_length_scale])\n",
    "\n",
    "# Define bounds for the length scale (e.g., between 0.1 and 10)\n",
    "length_scale_bounds = np.array([[0.1, 10.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3a332-17f7-4b48-b268-78d04f9b51ab",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "27e4dc67-ae9a-4b93-b39c-af4372ebed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "optimal_num_components = np.argmax(cumulative_variance >= 0.95) + 1  # +1 because indices start at 0\n",
    "selected_directions_pca = pca.components_[:optimal_num_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5fea6320-7e1f-40da-a750-fedf8146fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the PCA directions are computed beforehand and stored in `selected_directions`\n",
    "def sliced_wasserstein_exponential_kernel_wrapper_pca(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    # Use the previously computed PCA directions\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09fe8d14-30cc-446b-88c3-b43a6f6efe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy\n",
    "gp_models_pca = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_pca,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "\n",
    "    gp_models_pca.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "887d7434-42ab-4b6f-a4a7-60a309210e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA W-2 Sigmoid Link – Accuracy: 0.9167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       1.00      0.86      0.92         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.95      0.92      0.92        36\n",
      "weighted avg       0.94      0.92      0.92        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid_pca = predict_probs_sigmoid(X_test_scaled, gp_models_pca)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid_pca = np.argmax(gp_probabilities_sigmoid_pca, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid_pca = accuracy_score(y_test_sparse, gp_predictions_sigmoid_pca)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'PCA W-2 Sigmoid Link – Accuracy: {gp_accuracy_sigmoid_pca:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "201f2037-cc5a-4763-9b43-ef6fba2f9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA W-2 Probit Link – Accuracy: 0.9167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       1.00      0.86      0.92         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.95      0.92      0.92        36\n",
      "weighted avg       0.94      0.92      0.92        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_pca = predict_probs_probit_with_variance(X_test_scaled, gp_models_pca)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_pca = np.argmax(gp_probabilities_probit_pca, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "gp_accuracy_probit_pca = accuracy_score(y_test_sparse, gp_predictions_probit_pca)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'PCA W-2 Probit Link – Accuracy: {gp_accuracy_probit_var:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0d0bf-2500-4892-82ce-96688f0b82ca",
   "metadata": {},
   "source": [
    "### Uniform Directions using Fibonacci Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffdf84a2-7800-4569-b506-4398b1557252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_directions(d, n_directions):\n",
    "    \"\"\"\n",
    "    Generates uniformly distributed directions on the unit sphere in d dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - d: int, dimensionality of the data.\n",
    "    - n_directions: int, number of directions to generate.\n",
    "\n",
    "    Returns:\n",
    "    - directions: (n_directions, d) ndarray, each row is a unit vector.\n",
    "    \"\"\"\n",
    "    if d == 2:\n",
    "        # Uniformly spaced angles around the circle\n",
    "        angles = np.linspace(0, 2 * np.pi, n_directions, endpoint=False)\n",
    "        directions = np.stack([np.cos(angles), np.sin(angles)], axis=1)\n",
    "    elif d == 3:\n",
    "        # Fibonacci lattice for uniform points on a sphere\n",
    "        indices = np.arange(0, n_directions, dtype=float) + 0.5\n",
    "        phi = np.arccos(1 - 2*indices/n_directions)\n",
    "        theta = np.pi * (1 + 5**0.5) * indices\n",
    "        x, y_dir, z = (np.cos(theta) * np.sin(phi),\n",
    "                      np.sin(theta) * np.sin(phi),\n",
    "                      np.cos(phi))\n",
    "        directions = np.stack([x, y_dir, z], axis=1)\n",
    "    else:\n",
    "        # For higher dimensions, generate random directions and normalize\n",
    "        directions = np.random.randn(n_directions, d)\n",
    "        directions /= np.linalg.norm(directions, axis=1, keepdims=True)\n",
    "    return directions\n",
    "\n",
    "d = X_train_scaled.shape[1]  # Dimensionality of the data\n",
    "selected_directions_uniform = generate_uniform_directions(d, n_directions=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e8f4394c-d209-4291-ad24-dfadbe46e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliced_wasserstein_exponential_kernel_wrapper_uniform(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel using uniform grid directions.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d10ffd5-ef8e-4d98-b618-9ab185488325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy\n",
    "gp_models_uniform = []\n",
    "num_classes = 10 \n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_uniform,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "\n",
    "    gp_models_uniform.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "da827108-903e-4962-ad53-a10f68fad177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform W-2 Sigmoid Link – Accuracy: 0.8889\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       0.67      1.00      0.80         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       0.75      1.00      0.86         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       1.00      0.71      0.83         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.89        36\n",
      "   macro avg       0.91      0.90      0.89        36\n",
      "weighted avg       0.92      0.89      0.89        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid_uniform = predict_probs_sigmoid(X_test_scaled, gp_models_uniform)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid_uniform = np.argmax(gp_probabilities_sigmoid_uniform, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid_uniform = accuracy_score(y_test_sparse, gp_predictions_sigmoid_uniform)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Uniform W-2 Sigmoid Link – Accuracy: {gp_accuracy_sigmoid_uniform:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid_uniform))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43367960-1299-4d28-8f4b-3238d61542a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform W-2 Probit Link – Accuracy: 0.8889\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.71      1.00      0.83         5\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       0.67      1.00      0.80         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       0.75      1.00      0.86         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       1.00      0.71      0.83         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.89        36\n",
      "   macro avg       0.91      0.90      0.89        36\n",
      "weighted avg       0.92      0.89      0.89        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_uniform = predict_probs_probit_with_variance(X_test_scaled, gp_models_uniform)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_uniform = np.argmax(gp_probabilities_probit_uniform, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "gp_accuracy_probit_uniform = accuracy_score(y_test_sparse, gp_predictions_probit_uniform)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Uniform W-2 Probit Link – Accuracy: {gp_accuracy_probit_uniform:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_uniform))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c61fc0-87d5-4840-84c6-1e8bb9dd4c0e",
   "metadata": {},
   "source": [
    "### Orthogonal Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5590197b-eee6-4f63-b3f5-a0e364a04af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orthogonal_directions(dimension, n_directions):\n",
    "    \"\"\"\n",
    "    Generates k mutually orthogonal unit vectors in d-dimensional space using QR decomposition.\n",
    "\n",
    "    Parameters:\n",
    "    - d: int, dimensionality of the data.\n",
    "    - k: int, number of orthogonal directions to generate (k <= d).\n",
    "\n",
    "    Returns:\n",
    "    - directions: (k, d) ndarray, each row is a unit vector.\n",
    "    \"\"\"\n",
    "    if k > d:\n",
    "        raise ValueError(\"Number of directions k cannot exceed dimensionality d.\")\n",
    "    \n",
    "    # Step 1: Generate a random d x k matrix\n",
    "    random_matrix = np.random.randn(d, k)\n",
    "    \n",
    "    # Step 2: Perform QR decomposition\n",
    "    Q, R = np.linalg.qr(random_matrix)\n",
    "    \n",
    "    # Step 3: Extract the first k columns as orthogonal directions\n",
    "    directions = Q[:, :k].T  # Shape: (k, d)\n",
    "    \n",
    "    return directions\n",
    "\n",
    "d = X_train_scaled.shape[1]\n",
    "selected_directions_orthogonal = generate_orthogonal_directions(d, n_directions=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c5859eb8-bef1-4d72-9f75-1a757d7b4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliced_wasserstein_exponential_kernel_wrapper_orthogonal(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel using orthogonal directions.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_orthogonal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8bcb184-7c11-44d5-9344-ce5744a302d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy with Orthogonal Directions\n",
    "gp_models_orthogonal = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "    \n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "    \n",
    "    # Initialize GP model with the orthogonal directions-based kernel\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_orthogonal,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "    \n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "    \n",
    "    gp_models_orthogonal.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9853b712-b586-4c6a-b7dc-21f8bec58f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal W-2 Sigmoid Link – Accuracy: 0.9167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.83      1.00      0.91         5\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       0.60      1.00      0.75         3\n",
      "           8       1.00      0.57      0.73         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.94      0.96      0.94        36\n",
      "weighted avg       0.94      0.92      0.91        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid_orthogonal = predict_probs_sigmoid(X_test_scaled, gp_models_orthogonal)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid_orthogonal = np.argmax(gp_probabilities_sigmoid_orthogonal, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid_orthogonal = accuracy_score(y_test_sparse, gp_predictions_sigmoid_orthogonal)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Orthogonal W-2 Sigmoid Link – Accuracy: {gp_accuracy_sigmoid_orthogonal:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid_orthogonal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2215f650-4083-4b9d-afa6-37d697e1e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal W-2 Probit Link – Accuracy: 0.9167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       0.83      1.00      0.91         5\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       0.60      1.00      0.75         3\n",
      "           8       1.00      0.57      0.73         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.94      0.96      0.94        36\n",
      "weighted avg       0.94      0.92      0.91        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_orthogonal = predict_probs_probit_with_variance(X_test_scaled, gp_models_orthogonal)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_orthogonal = np.argmax(gp_probabilities_probit_orthogonal, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "gp_accuracy_probit_orthogonal = accuracy_score(y_test_sparse, gp_predictions_probit_orthogonal)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Orthogonal W-2 Probit Link – Accuracy: {gp_accuracy_probit_orthogonal:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_orthogonal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c2d68-def6-475b-897d-81bca8ca48f9",
   "metadata": {},
   "source": [
    "### Randomized Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "69ecbb99-0cd2-4bb6-a2af-d1d58656dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_directions(dimensions, n_directions, seed=42):\n",
    "    \"\"\"\n",
    "    Generates k randomly distributed unit vectors in d-dimensional space.\n",
    "\n",
    "    Parameters:\n",
    "    - d: int, dimensionality of the data.\n",
    "    - k: int, number of random directions to generate.\n",
    "    - seed: int, random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - directions: (k, d) ndarray, each row is a unit vector.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)  # For reproducibility\n",
    "    random_vectors = np.random.randn(k, d)  # Shape: (k, d)\n",
    "    norms = np.linalg.norm(random_vectors, axis=1, keepdims=True)  # Shape: (k, 1)\n",
    "    directions = random_vectors / norms  # Normalize to unit vectors\n",
    "    return directions\n",
    "\n",
    "d = X_train_scaled.shape[1]  \n",
    "selected_directions_random = generate_random_directions(d, n_directions=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f03fd4f-8bcf-4001-a91b-fc1a9be53219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliced_wasserstein_exponential_kernel_wrapper_random(x1, x2, hyperparameters):\n",
    "    \"\"\"\n",
    "    Wrapper function to match the expected signature for the GP kernel using randomized directions.\n",
    "\n",
    "    Parameters:\n",
    "    - x1: (n1, d) ndarray\n",
    "    - x2: (n2, d) ndarray\n",
    "    - hyperparameters: ndarray, contains [length_scale]\n",
    "\n",
    "    Returns:\n",
    "    - kernel_matrix: (n1, n2) ndarray\n",
    "    \"\"\"\n",
    "    return sliced_wasserstein_exponential_kernel_directions(x1, x2, hyperparameters, selected_directions_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef01e6b5-e331-488f-bdaa-6f51f5f18d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP model for class 0\n",
      "GP model for class 0 trained.\n",
      "\n",
      "Training GP model for class 1\n",
      "GP model for class 1 trained.\n",
      "\n",
      "Training GP model for class 2\n",
      "GP model for class 2 trained.\n",
      "\n",
      "Training GP model for class 3\n",
      "GP model for class 3 trained.\n",
      "\n",
      "Training GP model for class 4\n",
      "GP model for class 4 trained.\n",
      "\n",
      "Training GP model for class 5\n",
      "GP model for class 5 trained.\n",
      "\n",
      "Training GP model for class 6\n",
      "GP model for class 6 trained.\n",
      "\n",
      "Training GP model for class 7\n",
      "GP model for class 7 trained.\n",
      "\n",
      "Training GP model for class 8\n",
      "GP model for class 8 trained.\n",
      "\n",
      "Training GP model for class 9\n",
      "GP model for class 9 trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Initialize and Train GP Models for Each Class using One-vs-Rest Strategy with Random Directions\n",
    "gp_models_random = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}\")\n",
    "    \n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train_sparse == class_label).astype(float)\n",
    "    \n",
    "    # Initialize GP model with the random directions-based kernel\n",
    "    gp_model = GP(\n",
    "        X_train_scaled,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=sliced_wasserstein_exponential_kernel_wrapper_random,\n",
    "        noise_variances=np.ones_like(y_train_binary) * 0.25 + 1e-6  # Noise variance\n",
    "    )\n",
    "    \n",
    "    # Train the GP model using MCMC\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=100,\n",
    "        tolerance=1e-3,  \n",
    "    )\n",
    "    \n",
    "    gp_models_random.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0fc3a631-fc49-4db5-903c-19210bb733a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random W-2 Sigmoid Link – Accuracy: 0.8611\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.62      1.00      0.77         5\n",
      "           2       0.67      0.67      0.67         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       1.00      0.57      0.73         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.91      0.89      0.89        36\n",
      "weighted avg       0.90      0.86      0.86        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Sigmoid Link Function\n",
    "gp_probabilities_sigmoid_random = predict_probs_sigmoid(X_test_scaled, gp_models_random)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_sigmoid_random = np.argmax(gp_probabilities_sigmoid_random, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "gp_accuracy_sigmoid_random = accuracy_score(y_test_sparse, gp_predictions_sigmoid_random)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Random W-2 Sigmoid Link – Accuracy: {gp_accuracy_sigmoid_random:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_sigmoid_random))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11914e76-4172-4096-9782-ababa722bb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random W-2 Probit Link – Accuracy: 0.8611\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.62      1.00      0.77         5\n",
      "           2       0.67      0.67      0.67         3\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         3\n",
      "           8       1.00      0.57      0.73         7\n",
      "           9       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.91      0.89      0.89        36\n",
      "weighted avg       0.90      0.86      0.86        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict Probabilities using Probit Link with Variance\n",
    "gp_probabilities_probit_random = predict_probs_probit_with_variance(X_test_scaled, gp_models_random)\n",
    "\n",
    "# Predict Class Labels by Selecting the Class with the Highest Probability\n",
    "gp_predictions_probit_random = np.argmax(gp_probabilities_probit_random, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "\n",
    "gp_accuracy_probit_random = accuracy_score(y_test_sparse, gp_predictions_probit_random)\n",
    "\n",
    "# 9. Evaluate the Classifier\n",
    "print(f'Random W-2 Probit Link – Accuracy: {gp_accuracy_probit_random:.4f}\\n')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test_sparse, gp_predictions_probit_random))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c991189-b10e-46af-9159-6804708e5e66",
   "metadata": {},
   "source": [
    "### Max Margin Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79934952-7934-479a-a4f8-bca4fb90d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_margin_loss(y_true, prob_matrix):\n",
    "    \"\"\"\n",
    "    Computes the Custom Margin Ranking Loss.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True class labels. Shape: (n_samples,)\n",
    "    - prob_matrix (np.ndarray): Predicted probabilities for each class.\n",
    "                                 Shape: (n_samples, num_classes)\n",
    "\n",
    "    Returns:\n",
    "    - average_loss (float): The average loss over all samples.\n",
    "    \"\"\"\n",
    "    # Ensure y_true is a NumPy array\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Check if the number of samples matches\n",
    "    if prob_matrix.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(f\"Number of samples in prob_matrix ({prob_matrix.shape[0]}) does not match number of true labels ({y_true.shape[0]}).\")\n",
    "    \n",
    "    # Determine the predicted classes\n",
    "    y_pred = np.argmax(prob_matrix, axis=1)\n",
    "    \n",
    "    # Extract the predicted class probabilities\n",
    "    prob_pred = prob_matrix[np.arange(len(y_true)), y_pred]\n",
    "    \n",
    "    # Extract the true class probabilities\n",
    "    prob_correct = prob_matrix[np.arange(len(y_true)), y_true]\n",
    "    \n",
    "    # Compute loss: absolute difference if incorrect, else 0\n",
    "    loss = np.where(y_pred == y_true, 0, np.abs(prob_pred - prob_correct))\n",
    "    \n",
    "    # Compute the average loss\n",
    "    average_loss = np.mean(loss)\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63cb0c30-cfc7-47ad-bcd0-06e6e69564c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probit Random Loss: 0.0050\n",
      "Probit PCA Loss: 0.0039\n",
      "Probit Uniform Loss: 0.0038\n",
      "Probit Orthogonal Loss: 0.0040\n",
      "Sigmoid Random Loss: 0.0033\n",
      "Sigmoid PCA Loss: 0.0026\n",
      "Sigmoid Uniform Loss: 0.0025\n",
      "Sigmoid Orthogonal Loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "probabilities_dict = {\n",
    "    'Probit Random': gp_probabilities_probit_random,\n",
    "    'Probit PCA': gp_probabilities_probit_pca,\n",
    "    'Probit Uniform': gp_probabilities_probit_uniform,\n",
    "    'Probit Orthogonal': gp_probabilities_probit_orthogonal,\n",
    "    'Sigmoid Random': gp_probabilities_sigmoid_random,\n",
    "    'Sigmoid PCA': gp_probabilities_sigmoid_pca,\n",
    "    'Sigmoid Uniform': gp_probabilities_sigmoid_uniform,\n",
    "    'Sigmoid Orthogonal': gp_probabilities_sigmoid_orthogonal\n",
    "}\n",
    "\n",
    "# Loop through each of the models and calculate the loss\n",
    "for model_name, probabilities in probabilities_dict.items():\n",
    "    loss = custom_margin_loss(y_test_sparse, probabilities)\n",
    "    print(f\"{model_name} Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
