{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8efab03-62d4-4e8f-b153-d5be591558be",
   "metadata": {},
   "source": [
    "# Handwritten Digit Classification\n",
    "### Gaussian Process Classification\n",
    "\n",
    "#### Dataset Description ([Link to Data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html))\n",
    "\n",
    "Each entry corresponds to one hand-written digit on 8x8 pixels. The dataset contains 1797 samples, with about 180 samples for each of the 10 classes (0-9). A Gaussian Process Classification using the Jensen-Shannon Metric with an exponential kernels, where 10 models are trained for a One-v-Rest approach. We use PCA for direction optimizationand use both the probit link function to convert the regression to classes. This example should replicated without too much difficulty or time. Instead of training on global optimization, use an MCMC with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8a2c6c-4f8a-4e69-b3b0-bf4c1904d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from fvgp import GP\n",
    "from fvgp.gp_kernels import exponential_kernel\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b511908-2e5c-436c-87f4-aa26597b635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Preprocess the Digits Dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X = X[::10]\n",
    "y = y[::10]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalize the data to resemble probability distributions\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = (X_train[i] - np.min(X_train[i])) + 1e-8\n",
    "    X_train[i] = X_train[i] / np.sum(X_train[i])\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = (X_test[i] - np.min(X_test[i])) + 1e-8\n",
    "    X_test[i] = X_test[i] / np.sum(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ad6fd8-4c5e-440e-8921-2295a5501707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Sinkhorn Distance Function\n",
    "def sinkhorn_distance(p, q, M, reg):\n",
    "    '''\n",
    "    Compute the Sinkhorn distance between distributions p and q with cost matrix M and regularization reg\n",
    "    '''\n",
    "    # Ensure that p and q are numpy arrays\n",
    "    p = np.asarray(p, dtype=np.float64)\n",
    "    q = np.asarray(q, dtype=np.float64)\n",
    "    # Compute the Sinkhorn distance\n",
    "    sinkhorn_dist = ot.sinkhorn2(p, q, M, reg)\n",
    "    return sinkhorn_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91489580-2bf0-4de4-a6e8-21cbd12c4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute Sinkhorn Distance Matrices with Caching\n",
    "def compute_Sinkhorn_matrix(X1, X2, M, reg):\n",
    "    n1 = X1.shape[0]\n",
    "    n2 = X2.shape[0]\n",
    "    Sinkhorn_matrix = np.zeros((n1, n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            Sinkhorn_matrix[i, j] = sinkhorn_distance(X1[i], X2[j], M, reg)\n",
    "    return Sinkhorn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a08a86d-820b-4dd1-a67e-dce59a6bc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare Cost Matrix for Sinkhorn Distance\n",
    "# Positions of the pixels in the 8x8 grid\n",
    "positions = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        positions.append([i, j])\n",
    "positions = np.array(positions)  # shape (64, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769d62a6-e080-4172-b3d2-885c1b3579a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Sinkhorn distance matrices...\n",
      "Finished train x train\n",
      "Finished test x test\n",
      "Finished train x test\n",
      "Sinkhorn distance matrices computed.\n"
     ]
    }
   ],
   "source": [
    "# Compute the cost matrix (squared Euclidean distance between pixels)\n",
    "M = cdist(positions, positions, 'sqeuclidean')  # shape (64, 64)\n",
    "\n",
    "# Regularization parameter for Sinkhorn distance\n",
    "reg = 0.1\n",
    "\n",
    "print(\"Computing Sinkhorn distance matrices...\")\n",
    "Sinkhorn_X_train = compute_Sinkhorn_matrix(X_train, X_train, M, reg)       # Training vs. Training\n",
    "print(\"Finished train x train\")\n",
    "Sinkhorn_X_test = compute_Sinkhorn_matrix(X_test, X_test, M, reg)          # Testing vs. Testing\n",
    "print(\"Finished test x test\")\n",
    "Sinkhorn_X_train_test = compute_Sinkhorn_matrix(X_train, X_test, M, reg)   # Training vs. Testing\n",
    "print(\"Finished train x test\")\n",
    "print(\"Sinkhorn distance matrices computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09ef87bb-02ba-466c-ae50-a7cc57e0b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define the GP Kernel Function Using Precomputed Sinkhorn Matrices\n",
    "def Sinkhorn_kernel(X1, X2, hyperparameters):\n",
    "    length_scale = hyperparameters[0]\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    if len(X1) == n_train and len(X2) == n_train:\n",
    "        K = exponential_kernel(Sinkhorn_X_train, length_scale)\n",
    "    elif len(X1) == n_test and len(X2) == n_test:\n",
    "        K = exponential_kernel(Sinkhorn_X_test, length_scale)\n",
    "    elif len(X1) == n_train and len(X2) == n_test:\n",
    "        K = exponential_kernel(Sinkhorn_X_train_test, length_scale)\n",
    "    elif len(X1) == n_test and len(X2) == n_train:\n",
    "        K = exponential_kernel(Sinkhorn_X_train_test.T, length_scale)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input sizes for X1 and X2.\")\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0abcb95-b7c0-4a64-ba69-5f0979d98996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize Hyperparameters and Bounds\n",
    "initial_length_scale = 1.0\n",
    "init_hyperparameters = np.array([initial_length_scale])\n",
    "\n",
    "# Define bounds for the length scale\n",
    "length_scale_bounds = np.array([[0.1, 10.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9e5637-c429-4f25-bc31-905c2c933ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GP models...\n",
      "Training GP model for class 0...\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "135-th leading minor of the array is not positive definite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m gp_model \u001b[38;5;241m=\u001b[39m GP(\n\u001b[1;32m     12\u001b[0m     X_train,\n\u001b[1;32m     13\u001b[0m     y_train_binary,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     noise_variances\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(y_train_binary)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m  \u001b[38;5;66;03m# Noise variance\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the GP model (optimize hyperparameters)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mgp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_scale_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmcmc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m gp_models\u001b[38;5;241m.\u001b[39mappend(gp_model)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGP model for class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trained.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/fvgp/gp.py:508\u001b[0m, in \u001b[0;36mGP.train\u001b[0;34m(self, hyperparameter_bounds, objective_function, objective_function_gradient, objective_function_hessian, init_hyperparameters, method, pop_size, tolerance, max_iter, local_optimizer, global_optimizer, constraints, dask_client)\u001b[0m\n\u001b[1;32m    505\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective function: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, objective_function)\n\u001b[1;32m    506\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, method)\n\u001b[0;32m--> 508\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_function_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_function_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_function_hessian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_hyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdask_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdask_client\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39mupdate_hyperparameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mx_data, hyperparameters)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mx_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39my_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnoise_variances,\n\u001b[1;32m    526\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior\u001b[38;5;241m.\u001b[39mhyperparameters)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/fvgp/gp_training.py:90\u001b[0m, in \u001b[0;36mGPtraining.train\u001b[0;34m(self, objective_function, objective_function_gradient, objective_function_hessian, hyperparameter_bounds, init_hyperparameters, method, pop_size, tolerance, max_iter, local_optimizer, global_optimizer, constraints, dask_client)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     17\u001b[0m           objective_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m           objective_function_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m           constraints\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m     29\u001b[0m           dask_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    This function finds the maximum of the log marginal likelihood and therefore trains the GP (synchronously).\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    This can be done on a remote cluster/computer by specifying the method to be 'hgdl' and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m        `dask.distributed.Client` instance is constructed.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective_function_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective_function_hessian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_hyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparameter_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdask_client\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hyperparameters\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/fvgp/gp_training.py:375\u001b[0m, in \u001b[0;36mGPtraining._optimize_log_likelihood\u001b[0;34m(self, objective_function, objective_function_gradient, objective_function_hessian, starting_hps, hp_bounds, method, max_iter, pop_size, tolerance, constraints, local_optimizer, global_optimizer, dask_client)\u001b[0m\n\u001b[1;32m    373\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCMC started in fvGP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    374\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbounds are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, hp_bounds)\n\u001b[0;32m--> 375\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmcmc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_hps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribution mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcmc_info \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/fvgp/mcmc.py:106\u001b[0m, in \u001b[0;36mmcmc\u001b[0;34m(likelihood_fn, bounds, x0, n_updates, prior_args, info, prior_fn, prop_Sigma, adapt_cov, return_prop_Sigma_trace, r_opt, c_0, c_1, K)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Initialize Metropolis\u001b[39;00m\n\u001b[1;32m    105\u001b[0m theta \u001b[38;5;241m=\u001b[39m x0\n\u001b[0;32m--> 106\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mlikelihood_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m prior \u001b[38;5;241m=\u001b[39m prior_fn(theta, prior_args)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#########################################################\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Begin main loop\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/fvgp/gp_marginal_density.py:219\u001b[0m, in \u001b[0;36mGPMarginalDensity.log_likelihood\u001b[0;34m(self, hyperparameters)\u001b[0m\n\u001b[1;32m    217\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_obj\u001b[38;5;241m.\u001b[39mcompute_mean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_obj\u001b[38;5;241m.\u001b[39mx_data, hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters)\n\u001b[1;32m    218\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Prior mean computed after \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st)\n\u001b[0;32m--> 219\u001b[0m     KVinvY, KVlogdet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_new_KVlogdet_KVinvY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   KVinvY and logdet computed after \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st)\n\u001b[1;32m    222\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/fvgp/gp_marginal_density.py:150\u001b[0m, in \u001b[0;36mGPMarginalDensity.compute_new_KVlogdet_KVinvY\u001b[0;34m(self, K, V, m)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo mode in gp2Scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     Chol_factor \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_Chol_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     KVinvY \u001b[38;5;241m=\u001b[39m calculate_Chol_solve(Chol_factor, y_mean)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(y_mean))\n\u001b[1;32m    152\u001b[0m     KVlogdet \u001b[38;5;241m=\u001b[39m calculate_Chol_logdet(Chol_factor)\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/fvgp/gp_lin_alg.py:33\u001b[0m, in \u001b[0;36mcalculate_Chol_factor\u001b[0;34m(M)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_Chol_factor\u001b[39m(M):\n\u001b[1;32m     32\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalculate_Chol_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     c, l \u001b[38;5;241m=\u001b[39m \u001b[43mcho_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtril(c)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/scipy/linalg/_decomp_cholesky.py:153\u001b[0m, in \u001b[0;36mcho_factor\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcho_factor\u001b[39m(a, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Compute the Cholesky decomposition of a matrix, to use in cho_solve\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     c, lower \u001b[38;5;241m=\u001b[39m \u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c, lower\n",
      "File \u001b[0;32m~/anaconda3/envs/gpCAM_env/lib/python3.12/site-packages/scipy/linalg/_decomp_cholesky.py:36\u001b[0m, in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     34\u001b[0m c, info \u001b[38;5;241m=\u001b[39m potrf(a1, lower\u001b[38;5;241m=\u001b[39mlower, overwrite_a\u001b[38;5;241m=\u001b[39moverwrite_a, clean\u001b[38;5;241m=\u001b[39mclean)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-th leading minor of the array is not positive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefinite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m info)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAPACK reported an illegal value in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m-\u001b[39minfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-th argument\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon entry to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOTRF\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: 135-th leading minor of the array is not positive definite"
     ]
    }
   ],
   "source": [
    "# 7. Train GP Models Using One-vs-Rest Strategy\n",
    "gp_models = []\n",
    "num_classes = 10  # Digits 0-9\n",
    "\n",
    "print(\"Training GP models...\")\n",
    "for class_label in range(num_classes):\n",
    "    print(f\"Training GP model for class {class_label}...\")\n",
    "    # Binary labels for the current class\n",
    "    y_train_binary = (y_train == class_label).astype(float)\n",
    "    # Initialize GP model\n",
    "    gp_model = GP(\n",
    "        X_train,\n",
    "        y_train_binary,\n",
    "        init_hyperparameters=init_hyperparameters,\n",
    "        gp_kernel_function=Sinkhorn_kernel,\n",
    "        noise_variances=np.zeros(len(y_train_binary)) + 1e-6  # Noise variance\n",
    "    )\n",
    "\n",
    "    # Train the GP model (optimize hyperparameters)\n",
    "    gp_model.train(\n",
    "        hyperparameter_bounds=length_scale_bounds,\n",
    "        method='mcmc',\n",
    "        max_iter=1000,\n",
    "        tolerance=1e-3,\n",
    "    )\n",
    "\n",
    "    gp_models.append(gp_model)\n",
    "    print(f\"GP model for class {class_label} trained.\")\n",
    "\n",
    "print(\"All GP models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c3df7-221d-4377-98e6-d0a951709208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define the Probit Link Function (Prefer over Logit, Gaussian Assumptions)\n",
    "def probit(mu, sigma2):\n",
    "    # Applies the probit function with variance adjustment.\n",
    "    adjusted_mu = mu / np.sqrt(1 + sigma2)\n",
    "    return norm.cdf(adjusted_mu)\n",
    "\n",
    "# Predict Probabilities Using the Trained GP Models\n",
    "def predict_probs(X_test, gp_models):\n",
    "    num_classes = len(gp_models)\n",
    "    n_test = X_test.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store means and variances\n",
    "    means = np.zeros((n_test, num_classes))\n",
    "    variances = np.zeros((n_test, num_classes))\n",
    "    \n",
    "    for class_label, gp_model in enumerate(gp_models):\n",
    "        # Compute the posterior mean for the test data\n",
    "        posterior_mean = gp_model.posterior_mean(X_test)\n",
    "        mean = posterior_mean[\"f(x)\"]  # Extract mean predictions\n",
    "        means[:, class_label] = mean.flatten()\n",
    "        \n",
    "        # Compute the posterior variance for the test data\n",
    "        posterior_cov = gp_model.posterior_covariance(X_test, variance_only=True)\n",
    "        variance = posterior_cov[\"v(x)\"]  # Extract variances\n",
    "        variances[:, class_label] = variance.flatten()\n",
    "    \n",
    "    # Apply probit with variance to convert means and variances to probabilities\n",
    "    probabilities = probit(means, variances)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73509166-76ca-4852-b394-006df8246e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Predict Class Labels and Evaluate the Classifier\n",
    "probabilities = predict_probs(X_test, gp_models)\n",
    "\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f'\\nAccuracy: {accuracy:.0f}%')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20d0a4-af7f-4579-bbde-eae413485b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e4a08-9f4c-4fad-be75-447385bae62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79c716-75fa-4287-a059-0aa198bac65d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpCAM_env)",
   "language": "python",
   "name": "gpcam_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
